{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/sentiment_comp_qaie_pairs.pkl','rb') as f:\n",
    "    pairs = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12325\n",
      "12272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['æ™ºé€šè´¢ç»APPè®¯ï¼Œå¤ªå¹³æ´‹ç½‘ç»œ(00543)å‘å¸ƒå…¬å‘Šï¼Œå°†äº2023å¹´6æœˆ12æ—¥æ´¾å‘æˆªè‡³2022å¹´12æœˆ31æ—¥æ­¢å¹´åº¦çš„æœ«æœŸè‚¡æ¯æ¯è‚¡0.1å…ƒäººæ°‘å¸ã€‚\\n---\\nè¯·ä»ä¸Šæ–‡ä¸­æŠ½å–å‡ºæ‰€æœ‰å…¬å¸/æœºæ„ã€å¯¹åº”çš„åœ¨æœ¬æ–‡ä¸­çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æã€æ¶ˆæã€ä¸­æ€§ï¼‰ä»¥åŠåŸå› ã€‚\\nå¹¶ç”¨è¿™æ ·çš„æ ¼å¼è¿”å›ï¼š\\n{\"ORG\":..., \"sentiment\":..., \"reason\":...}',\n",
       " '{\"ORG\": \"å¤ªå¹³æ´‹ç½‘ç»œ\", \"sentiment\": \"ç§¯æ\", \"reason\": \"å®£å¸ƒæ´¾å‘è‚¡æ¯æ¯è‚¡0.1å…ƒäººæ°‘å¸\"}']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "pairs = [p for p in pairs if p[1] not in ['æ— ','']]\n",
    "print(len(pairs))\n",
    "\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/sentiment_comp_ie.json','w',encoding='utf8') as f:\n",
    "    for p in pairs:\n",
    "        line = {\"q\":p[0],\"a\":p[1]}\n",
    "        f.write(json.dumps(line,ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“ä¹±é¡ºåºé‡æ–°æ’åˆ—\n",
    "import json\n",
    "import random\n",
    "with open('data/sentiment_comp_ie_shuffled.json','w',encoding='utf8') as f:\n",
    "    random_pairs = random.sample(pairs,k=len(pairs))\n",
    "    for p in random_pairs:\n",
    "        line = {\"q\":p[0],\"a\":p[1]}\n",
    "        f.write(json.dumps(line,ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baichuan-7B inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½ç™¾å·å¤§æ¨¡å‹çœ‹çœ‹\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°è¯•ä»å¤§æ¨¡å‹ä¸­è·å–æƒ…æ„Ÿæ ‡ç­¾çš„æ¦‚ç‡å€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"é—®ï¼šä»Šå¤©å¿ƒæƒ…è¿˜çœŸä¸é”™ã€‚è¿™å¥è¯çš„æƒ…æ„Ÿæ˜¯æ€æ ·çš„ï¼Ÿç­”ï¼šè¿™å¥è¯çš„æƒ…æ„Ÿæ˜¯ï¼š\"\n",
    "inputs = tokenizer.encode(text, return_tensors='pt').to('cuda:0')\n",
    "# outputs = model.generate(inputs,max_new_tokens=50,do_sample=True)\n",
    "# output = outputs[0][len(inputs[0]):]\n",
    "# tokenizer.decode(output,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['æ¶ˆæ', 'è´Ÿé¢', 'éš¾è¿‡'] score(prob*100) = 0.2\n",
      "1 ['ç§¯æ', 'æ­£é¢', 'å¼€å¿ƒ'] score(prob*100) = 0.07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"ChatGPTçš„æ¨å‡ºå¯¹ç™¾åº¦çš„æœç´¢ä¸šåŠ¡äº§ç”Ÿçš„å¼ºçƒˆå†²å‡»ï¼Œæœç´¢å¼•æ“çš„ä½œç”¨æ€§åœ¨é™ä½\"\n",
    "\n",
    "prompt = f\"é—®ï¼š{text}è¿™å¥è¯çš„æƒ…æ„Ÿæ˜¯æ€æ ·çš„ï¼Ÿç­”ï¼šè¿™å¥è¯çš„æƒ…æ„Ÿæ˜¯ï¼š\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt').to('cuda:0')\n",
    "\n",
    "last_logits = model(inputs).logits[:,-1,:]\n",
    "probs = torch.softmax(last_logits, dim=-1)\n",
    "\n",
    "targets = {\n",
    "    0: ['æ¶ˆæ','è´Ÿé¢','éš¾è¿‡'],\n",
    "    1: ['ç§¯æ','æ­£é¢','å¼€å¿ƒ']\n",
    "}\n",
    "for key in targets:\n",
    "    target_probs = []\n",
    "    for w in targets[key]:\n",
    "        # å–å‡ºé bos çš„ç¬¬ä¸€ä¸ªtokenï¼ˆä¼¼ä¹æ— æ³•é€šè¿‡ add_special_tokens=False å»æ‰ï¼‰\n",
    "        idx = tokenizer.encode(w)[1]\n",
    "        current_prob = probs[0, idx].item()\n",
    "        # print(w, current_prob)\n",
    "        target_probs.append(current_prob)\n",
    "    print(key,targets[key],'score(prob*100) =',round(sum(target_probs)*100/len(target_probs),2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä¸è€çƒ¦ï¼Œæå…¶ä¸è€çƒ¦ã€‚ä¸ºä»€ä¹ˆä¼šä¸è€çƒ¦å‘¢ï¼Ÿå› ä¸ºæˆ‘ä»¬æ€»æ˜¯è§‰å¾—å¯¹æ–¹ä¸äº†è§£æƒ…å†µã€‚1 å…ˆäº†è§£ä¸€ä¸‹äº‹æƒ…çš„å§‹æœ«å§ï¼åœ¨è¯´è¿™å¥è¯çš„æ—¶å€™ï¼Œå¯¹æ–¹å¯èƒ½ä¼šè¯´ï¼šâ€œäº‹æƒ…ä¸æ˜¯è¿™æ ·å­çš„å‘€'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(inputs,max_new_tokens=50,do_sample=True)\n",
    "output = outputs[0][len(inputs[0]):]\n",
    "tokenizer.decode(output,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"weights/sentiment_comp_ie_shuffled_baichuan-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ChatGPTçš„æå‡ºå¯¹è°·å˜ã€ä¸‡åº¦çš„æœç´¢ä¸šåŠ¡äº§ç”Ÿå·¨å¤§æ‰“å‡»ï¼Œä¼ ç»Ÿæœç´¢å¼•æ“çš„ä½œç”¨æ€§é™ä½äº†ã€‚\n",
    "ä¸æ­¤åŒæ—¶ï¼ŒOChatï¼ŒLinguoç­‰æ–°å…´è¯­ä¹‰æœç´¢å…¬å¸ï¼Œè¿…é€Ÿæ¨å‡ºè‡ªå·±çš„ç±»ChatGPTæ¨¡å‹ï¼Œå¹¶ç»“åˆè¿›è‡ªå®¶æœç´¢å¼•æ“ï¼Œå—åˆ°äº†å¾ˆå¤šç”¨æˆ·çš„é’çã€‚\n",
    "è…¾åŠ¿ã€è‰¾é‡Œç­‰å…¬å¸è¡¨ç¤ºä¼šè¿…é€Ÿè·Ÿè¿›ChatGPTå’ŒAIGCçš„å‘å±•ï¼Œå¹¶é¢„è®¡åœ¨å¹´åº•å‰æ¨å‡ºè‡ªå·±çš„å¤§æ¨¡å‹ã€‚\n",
    "å¤§å‹å›¾ç‰‡ä¾›åº”å•†è§†è§‰ä¸­å›½ç§°ChatGPTå¯¹å…¬å¸ä¸šåŠ¡æš‚æ— å½±å“ï¼Œè¿˜åœ¨è§‚æœ›çŠ¶æ€ã€‚\n",
    "ï¼ˆæœ¬æ–‡å›¾ç‰‡æ¥è‡ªè§†è§‰ä¸­å›½ï¼Œä¸Šè§‚æ–°é—»ä¸ºæ‚¨æŠ¥é“ã€‚ï¼‰\n",
    "æ›´å¤šæŠ¥é“ï¼š\n",
    "- äºšç‰›é€Šå…¬å¸å…³äºAIGCçš„è¡¨æ€\n",
    "- å·¨ç¡¬å…¬å¸æ˜¨æ—¥åœ¨Aè‚¡ä¸Šå¸‚\n",
    "---\n",
    "è¯·ä»ä¸Šæ–‡ä¸­æŠ½å–å‡ºæ‰€æœ‰å…¬å¸ï¼Œä»¥åŠå¯¹åº”çš„åœ¨æœ¬æ–‡ä¸­çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æã€æ¶ˆæã€ä¸­æ€§ï¼‰ä»¥åŠåŸå› ã€‚\n",
    "è¯·ç”¨è¿™æ ·çš„æ ¼å¼è¿”å›ï¼š\n",
    "{\"ORG\":..., \"sentiment\":..., \"reason\":...}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. {\"ORG\":\"ç™¾åº¦\",\"SENTIMENT\":\"+20%\", \"REASON\":\"æ–°äº§å“å‘å¸ƒ/æ›´æ–°;æŠ€æœ¯çªç ´;è¡Œä¸šé¢†å…ˆåœ°ä½æå‡;å¸‚åœºä»½é¢æ‰©å¤§;è¥æ”¶å¢é•¿;è‚¡ä»·ä¸Šæ¶¨\"; }\n",
      "2. {\"ORG\":\"é˜¿é‡Œå·´å·´é›†å›¢\",\"SENTIMENT\":\"-50%\", \"REASON\":\"ç«äº‰å¯¹æ‰‹å‘å±•è¿…çŒ›;ä¸šç»©ä¸‹æ»‘;å¸‚å€¼ç¼©æ°´;è‚¡ä»·ä¸‹è·Œ\"; }\n",
      "3. {\"ORG\":\"è…¾è®¯æ§è‚¡æœ‰é™å…¬å¸\",\"SENTIMENT\":\"+40%\", \"REASON\":\"æ–°äº§å“å‘å¸ƒ\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "inputs = inputs.to('cuda:0')\n",
    "output = model.generate(**inputs, max_new_tokens=128,repetition_penalty=1.1, streamer=streamer)\n",
    "# æ¨¡å‹éå¸¸è‡ªä¿¡ï¼ï¼ˆç±»ä¼¼äºæ¨¡å‹è‡ªåŠ¨çº é”™çš„èƒ½åŠ›ï¼‰åŒæ—¶å¹»è§‰ååˆ†ä¸¥é‡ï¼Œå“ˆå“ˆå“ˆï¼ˆå½“ç„¶ï¼Œè¿™åªæ˜¯è®­ç»ƒäº†200stepsï¼‰"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "device=torch.device(2)\n",
    "\n",
    "model_path = \"THUDM/chatglm-6b\"\n",
    "# model_glm = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().to(device)\n",
    "model_glm = AutoModel.from_pretrained(model_path, trust_remote_code=True,device_map='auto')\n",
    "tokenizer_glm = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LoRA\n",
    "model_glm = PeftModel.from_pretrained(model_glm, \"weights/sentiment_comp_ie\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘ä¸ç¡®å®šæ‚¨æ‰€æŒ‡çš„â€œè‰¾é‡Œå·´å·´å…¬å¸â€æ˜¯æŒ‡å“ªä¸ªå…¬å¸ã€‚å¦‚æœæ‚¨èƒ½æä¾›æ›´å¤šä¸Šä¸‹æ–‡æˆ–ä¿¡æ¯,æˆ‘å°†å°½åŠ›å›ç­”æ‚¨çš„é—®é¢˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer_glm,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer_glm(\"æˆ‘ä»¬è¯´çš„è‰¾é‡Œå·´å·´å…¬å¸ï¼ŒæŒ‡çš„æ˜¯\", return_tensors='pt')\n",
    "inputs = inputs.to('cuda:2')\n",
    "output = model_glm.generate(**inputs, max_new_tokens=1024,repetition_penalty=1.1, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "èèµ„èµ„é‡‘å°†ç”¨äºå®Œå–„ç”Ÿæˆå¼AIæ–°å¼•æ“çš„æ„å»ºã€‚\n",
      "\n",
      "åˆ›ä¸šé‚¦è·æ‚‰ï¼Œè‡ªç„¶è¯­è¨€æŠ€æœ¯AIæœåŠ¡å•†ç«¹é—´æ™ºèƒ½å®£å¸ƒå·²å®ŒæˆD2è½®èèµ„ï¼Œç”±é‡‘æµ¦æŠ•èµ„ã€é‡‘åº“èµ„æœ¬ã€æ±Ÿè‹æ–‡æŠ•ã€éš½èµèµ„æœ¬ç­‰è”åˆæŠ•èµ„ã€‚è‡³ä»Šï¼Œç«¹é—´æ™ºèƒ½å·²ç´¯è®¡å®Œæˆ7è½®èèµ„ï¼Œé™†ç»­å¼•å…¥ç§‘æ²ƒæ–¯ã€äº‘æ™–èµ„æœ¬ã€ä¸­é“¶å›½é™…ç­‰è‚¡ä¸œæœºæ„ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œç«¹é—´æ™ºèƒ½å·²ç»æ­£å¼æ¨å‡ºè¿ç”¨ç±»ChatGPTæŠ€æœ¯çš„æˆç†ŸAIGCäº§å“ï¼Œå¹¶å³å°†å¼€å¯æ–°ä¸€è½®èèµ„ã€‚\n",
      "\n",
      "èèµ„èµ„é‡‘å°†ç”¨äºå®Œå–„ç”Ÿæˆå¼AIæ–°å¼•æ“çš„æ„å»ºï¼ŒæŠŠä»¥ChatGPTä¸ºä»£è¡¨çš„å¤§è¯­è¨€æ¨¡å‹å’ŒAIGCæŠ€æœ¯å…¨é¢èå…¥ç«¹é—´äº§å“ä½“ç³»ï¼Œç»“åˆå¤§å°æ¨¡å‹æ„å»ºåŒå¼•æ“é©±åŠ¨äº§å“è¿­ä»£å’ŒæŠ€æœ¯å‡çº§ï¼Œå¹³è¡¡å¤§å°æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ï¼Œå…¨é¢ç„•æ–°äº§å“åŠŸèƒ½å’ŒæœåŠ¡æ¨¡å¼ï¼Œå¹¶æ­£å¼å°†äº§å“åŒæ­¥æ¨å‘æµ·å¤–å¸‚åœºï¼Œæˆä¸ºæœåŠ¡å…¨çƒä¼ä¸šå’Œç”¨æˆ·çš„è·¨å›½NLPèƒ½åŠ›å‚å•†ã€‚\n",
      "\n",
      "ç«¹é—´æ™ºèƒ½ç”±å‰å¾®è½¯ï¼ˆäºšæ´²ï¼‰äº’è”ç½‘å·¥ç¨‹é™¢å‰¯é™¢é•¿ç®€ä»è´¤äº2015å¹´åˆ›åŠï¼Œè‡´åŠ›äºä»¥è‡ªç„¶è¯­è¨€å¤„ç†ã€æƒ…æ„Ÿè®¡ç®—ã€æ·±åº¦å­¦ä¹ ã€çŸ¥è¯†å·¥ç¨‹ã€æ–‡æœ¬å¤„ç†ç­‰äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸ºåŸºç¡€ï¼Œå°†AIèƒ½åŠ›æƒ åŠåƒè¡Œç™¾ä¸šã€‚\n",
      "\n",
      "---\n",
      "è¯·ä»ä¸Šæ–‡ä¸­æŠ½å–å‡ºæ‰€æœ‰å…¬å¸ï¼Œä»¥åŠå¯¹åº”çš„åœ¨æœ¬æ–‡ä¸­çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æã€æ¶ˆæã€ä¸­æ€§ï¼‰ä»¥åŠåŸå› ã€‚\n",
      "è¯·ç”¨è¿™æ ·çš„æ ¼å¼è¿”å›ï¼š\n",
      "{\"ORG\":..., \"sentiment\":..., \"reason\":...}\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizerè‡ªåŠ¨æŠŠä¸­æ–‡æ‹¬å·è½¬æˆè‹±æ–‡æ‹¬å·äº†...æ‰€ä»¥ä½ ç›´æ¥åœ¨è¾“å‡ºé‡Œé¢å¯èƒ½æ— æ³•ç›´æ¥åŒ¹é…åˆ° prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM2-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device='cuda:0')\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device_map='auto')\n",
    "# model = model.eval()\n",
    "# model\n",
    "# response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    "# print(response)\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hf_device_map\n",
    "model.hf_device_map['transformer.output_layer'] = model.hf_device_map['transformer.embedding']\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device_map=model.hf_device_map)\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer('ä½ å¥½',return_tensors='pt')\n",
    "res_glm = tokenizer_glm('ä½ å¥½',return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = res['input_ids'].clone()\n",
    "model(input_ids=res['input_ids'],labels=labels)\n",
    "\n",
    "# model_glm = model_glm.half()\n",
    "# labels_glm = res_glm['input_ids'].clone()\n",
    "# model_glm(input_ids=res_glm['input_ids'],labels=labels_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¯¹æ¯”ä¸€ä¸‹ ChatGLM è·Ÿ ChatGLM2 çš„ç»“æ„å·®åˆ«ï¼š**\n",
    "\n",
    "ChatGLM:\n",
    "```python\n",
    "ChatGLMForConditionalGeneration(\n",
    "  (transformer): ChatGLMModel(\n",
    "    (word_embeddings): Embedding(130528, 4096)\n",
    "    (layers): ModuleList(\n",
    "      (0-27): 28 x GLMBlock(\n",
    "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "        (attention): SelfAttention(\n",
    "          (rotary_emb): RotaryEmbedding()\n",
    "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
    "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        )\n",
    "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GLU(\n",
    "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
    "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
    "```\n",
    "\n",
    "ChatGLM2:\n",
    "```python\n",
    "ChatGLMForConditionalGeneration(\n",
    "  (transformer): ChatGLMModel(\n",
    "    (embedding): Embedding(\n",
    "      (word_embeddings): Embedding(65024, 4096)   # <-- smaller vocab size\n",
    "    )\n",
    "    (rotary_pos_emb): RotaryEmbedding()\n",
    "    (encoder): GLMTransformer(\n",
    "      (layers): ModuleList(\n",
    "        (0-27): 28 x GLMBlock(\n",
    "          (input_layernorm): RMSNorm()   # <-- LayerNorm to RMSNorm\n",
    "          (self_attention): SelfAttention(\n",
    "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)   # <-- smaller attention out_features\n",
    "            (core_attention): CoreAttention(\n",
    "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          )\n",
    "          (post_attention_layernorm): RMSNorm()\n",
    "          (mlp): MLP(                # <-- GLU to MLP\n",
    "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
    "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "      (final_layernorm): RMSNorm()\n",
    "    )\n",
    "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)    # <-- smaller out_features\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=130528, bias=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glm.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=65024, bias=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ORG\": \"ChatGPT\", \"sentiment\": \"æ¶ˆæ\", \"reason\": \"ChatGPTçš„æå‡ºå¯¹è°·å˜ã€ä¸‡åº¦çš„æœç´¢ä¸šåŠ¡äº§ç”Ÿå·¨å¤§æ‰“å‡»ï¼Œä¼ ç»Ÿæœç´¢å¼•æ“çš„ä½œç”¨æ€§é™ä½äº†ã€‚\"},\n",
      "{\"ORG\": \"OChat\", \"sentiment\": \"ç§¯æ\", \"reason\": \"OChatï¼ŒLinguoç­‰æ–°å…´è¯­ä¹‰æœç´¢å…¬å¸ï¼Œè¿…é€Ÿæ¨å‡ºè‡ªå·±çš„ç±»ChatGPTæ¨¡å‹ï¼Œå¹¶ç»“åˆè¿›è‡ªå®¶æœç´¢å¼•æ“ï¼Œå—åˆ°äº†å¾ˆå¤šç”¨æˆ·çš„é’çã€‚\"},\n",
      "{\"ORG\": \"è…¾åŠ¿\", \"sentiment\": \"ç§¯æ\", \"reason\": \"è…¾åŠ¿ã€è‰¾é‡Œç­‰å…¬å¸è¡¨ç¤ºä¼šè¿…é€Ÿè·Ÿè¿›ChatGPTå’ŒAIGCçš„å‘å±•ï¼Œå¹¶é¢„è®¡åœ¨å¹´åº•å‰æ¨å‡ºè‡ªå·±çš„å¤§æ¨¡å‹ã€‚\"},\n",
      "{\"ORG\": \"è§†è§‰ä¸­å›½\", \"sentiment\": \"ä¸­æ€§\", \"reason\": \"å¤§å‹å›¾ç‰‡ä¾›åº”å•†è§†è§‰ä¸­å›½ç§°ChatGPTå¯¹å…¬å¸ä¸šåŠ¡æš‚æ— å½±å“ï¼Œè¿˜åœ¨è§‚æœ›çŠ¶æ€ã€‚\"},\n",
      "{\"ORG\": \"äºšç‰›é€Šå…¬å¸\", \"sentiment\": \"ä¸­æ€§\", \"reason\": \"äºšç‰›é€Šå…¬å¸å…³äºAIGCçš„è¡¨æ€ä¸­å¹¶æœªæåŠChatGPTå¯¹å…¬å¸ä¸šåŠ¡çš„å½±å“ã€‚\"},\n",
      "{\"ORG\": \"å·¨ç¡¬å…¬å¸\", \"sentiment\": \"ä¸­æ€§\", \"reason\": \"å·¨ç¡¬å…¬å¸æ˜¨æ—¥åœ¨Aè‚¡ä¸Šå¸‚ä¸ChatGPTå¯¹å…¬å¸ä¸šåŠ¡çš„å½±å“æ— å…³ã€‚\"}\n"
     ]
    }
   ],
   "source": [
    "print(model.chat(tokenizer,query=text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"weights/sentiment_comp_ie_chatglm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ORG\": \"è°·å˜\", \"sentiment\": \"æ¶ˆæ\", \"reason\": \"ChatGPTçš„æå‡ºå¯¹å…¶æœç´¢ä¸šåŠ¡äº§ç”Ÿå·¨å¤§æ‰“å‡»\"}\n",
      "{\"ORG\": \"ä¸‡åº¦\", \"sentiment\": \"æ¶ˆæ\", \"reason\": \"ä¼ ç»Ÿæœç´¢å¼•æ“çš„ä½œç”¨æ€§é™ä½äº†\"}\n",
      "{\"ORG\": \"OChat\", \"sentiment\": \"ç§¯æ\", \"reason\": \"è¿…é€Ÿæ¨å‡ºè‡ªå·±çš„ç±»ChatGPTæ¨¡å‹ï¼Œå¹¶ç»“åˆè¿›è‡ªå®¶æœç´¢å¼•æ“ï¼Œå—åˆ°äº†å¾ˆå¤šç”¨æˆ·çš„é’ç\"}\n",
      "{\"ORG\": \"Linguo\", \"sentiment\": \"ç§¯æ\", \"reason\": \"è¿…é€Ÿæ¨å‡ºè‡ªå·±çš„ç±»ChatGPTæ¨¡å‹ï¼Œå¹¶ç»“åˆè¿›è‡ªå®¶æœç´¢å¼•æ“ï¼Œå—åˆ°äº†å¾ˆå¤šç”¨æˆ·çš„é’ç\"}\n",
      "{\"ORG\": \"è…¾åŠ¿\", \"sentiment\": \"ç§¯æ\", \"reason\": \"ä¼šè¿…é€Ÿè·Ÿè¿›ChatGPTå’ŒAIGCçš„å‘å±•ï¼Œå¹¶é¢„è®¡åœ¨å¹´åº•å‰æ¨å‡ºè‡ªå·±çš„å¤§æ¨¡å‹\"}\n",
      "{\"ORG\": \"è‰¾é‡Œ\", \"sentiment\": \"ç§¯æ\", \"reason\": \"ä¼šè¿…é€Ÿè·Ÿè¿›ChatGPTå’ŒAIGCçš„å‘å±•ï¼Œå¹¶é¢„è®¡åœ¨å¹´åº•å‰æ¨å‡ºè‡ªå·±çš„å¤§æ¨¡å‹\"}\n",
      "{\"ORG\": \"è§†è§‰ä¸­å›½\", \"sentiment\": \"ä¸­æ€§\", \"reason\": \"ç§°ChatGPTå¯¹å…¬å¸ä¸šåŠ¡æš‚æ— å½±å“ï¼Œè¿˜åœ¨è§‚æœ›çŠ¶æ€\"}\n"
     ]
    }
   ],
   "source": [
    "print(model.chat(tokenizer,text)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HC3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "hc3 = load_from_disk('data/hc3_chatgpt_qa_all')\n",
    "hc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'ç›—è´¼å¤©èµ‹ç›—è´¼æ€ä¹ˆåŠ å¤©èµ‹?çŸ¥é“å‘Šè¯‰ä¸€ä¸‹ä¸‹å•¦~~ ',\n",
       " 'answer': 'å¦‚æœä½ åœ¨ç©è§’è‰²æ‰®æ¼”æ¸¸æˆï¼ˆRPGï¼‰ï¼Œé‚£ä¹ˆä½ å¯èƒ½æ˜¯åœ¨é—®å¦‚ä½•åœ¨æ¸¸æˆä¸­ç»™ä½ çš„ç›—è´¼è§’è‰²åŠ å¤©èµ‹ã€‚å…·ä½“æ–¹æ³•å¯èƒ½å› æ¸¸æˆè€Œå¼‚ï¼Œä½†é€šå¸¸æœ‰ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š \\n1. åœ¨æ¸¸æˆå¼€å§‹æ—¶é€‰æ‹©ç›—è´¼å¤©èµ‹ï¼šåœ¨æ¸¸æˆå¼€å§‹æ—¶ï¼Œä½ å¯ä»¥é€‰æ‹©ä½ æƒ³è¦çš„å¤©èµ‹ã€‚è¿™é€šå¸¸æ˜¯é€šè¿‡é€‰æ‹©ä¸åŒçš„è§’è‰²ç§æ—æˆ–èŒä¸šæ¥å®ç°çš„ã€‚ \\n2. åœ¨æ¸¸æˆè¿›ç¨‹ä¸­è·å¾—å¤©èµ‹ï¼šåœ¨æ¸¸æˆè¿›ç¨‹ä¸­ï¼Œä½ å¯èƒ½ä¼šè·å¾—ä¸€äº›ä¸ç›—è´¼ç›¸å…³çš„å¤©èµ‹ã€‚è¿™å¯èƒ½æ˜¯é€šè¿‡å®Œæˆä»»åŠ¡ã€å‡çº§æˆ–è§£é”æ–°çš„æŠ€èƒ½æ¥å®ç°çš„ã€‚ \\n3. ä½¿ç”¨é“å…·æˆ–è£…å¤‡è·å¾—å¤©èµ‹ï¼šä½ å¯èƒ½ä¼šå‘ç°ä¸€äº›é“å…·æˆ–è£…å¤‡ï¼Œå®ƒä»¬å¯ä»¥èµ‹äºˆä½ ä¸€äº›ç›—è´¼å¤©èµ‹ã€‚è¿™äº›é“å…·æˆ–è£…å¤‡é€šå¸¸æ˜¯éšæœºç”Ÿæˆçš„ï¼Œæˆ–è€…æ˜¯ä½ åœ¨æ¸¸æˆè¿›ç¨‹ä¸­è·å¾—çš„å¥–åŠ±ã€‚ \\nå¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/hc3_chatgpt_zh_specific_qa.json','w',encoding='utf8') as f:\n",
    "    for i in range(len(hc3)):\n",
    "        # line = {'q':hc3[i]['question'], 'a':hc3[i]['answer']}\n",
    "        line = {'q':'é—®ï¼š'+hc3[i]['question'], 'a':'ç­”ï¼š'+hc3[i]['answer']}\n",
    "        line = json.dumps(line, ensure_ascii=False)\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True)\n",
    "import torch\n",
    "device = torch.device('cuda:2')\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True).to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "    inputs = tokenizer(\"é—®ï¼š\"+text+\"ç­”ï¼š\", return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:2')\n",
    "    output = model.generate(**inputs, max_new_tokens=128,\n",
    "                            do_sample=True,\n",
    "                            repetition_penalty=1.1, \n",
    "                            begin_suppress_tokens=[tokenizer.eos_token_id],\n",
    "                            # streamer=streamer\n",
    "                            )\n",
    "    output = output[0][inputs.input_ids.shape[1]:] # è¿™æ ·å¯ä»¥é˜²æ­¢è¾“å‡ºpromptéƒ¨åˆ†\n",
    "    return tokenizer.decode(output,skip_special_tokens=True)\n",
    "    # return output\n",
    "    \n",
    "# chat('ä½ æ€ä¹ˆæŠŠæ–­äº†çš„èºä¸é’‰é’»å‡ºæ¥ï¼Ÿ')\n",
    "# tokenizer.eos_token_id\n",
    "# tokenizer.encode('  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(model, \"weights/rulai_plus_baichuan-7B/\")\n",
    "model = PeftModel.from_pretrained(model, \"RLHF/weights/baichaun_rlhf_beyond_chinese_test_6step_40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ‰äº‹æƒ…æƒ³æ‰¾è€å…¬å•†é‡åˆä¸å¥½æ„æ€ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "output = chat('å“ï¼Œä»Šå¤©å¥½çƒ¦å•Š')\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len = 2500  ~ 9G\n",
    "len = 5000  ~ 40G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /root/anaconda3/envs/gby did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "A new version of the following files was downloaded from https://huggingface.co/internlm/internlm-chat-7b:\n",
      "- modeling_internlm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.26it/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:14<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('internlm/internlm-chat-7b', trust_remote_code=True, device_map=\"auto\") \n",
    "tokenizer = AutoTokenizer.from_pretrained('internlm/internlm-chat-7b', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLMForCausalLM(\n",
       "  (model): InternLMModel(\n",
       "    (embed_tokens): Embedding(103168, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x InternLMDecoderLayer(\n",
       "        (self_attn): InternLMAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (rotary_emb): InternLMRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): InternLMMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): InternLMRMSNorm()\n",
       "        (post_attention_layernorm): InternLMRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLMRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=103168, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŒ‰ç…§æƒ¯ä¾‹ï¼Œå…ˆè§‚å¯Ÿä¸€ä¸‹æ¨¡å‹ç»“æ„\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"æ­£å‘\":1,\"è´Ÿå‘\":0,\"æœªæåŠ\":0,\"ä¸­\":0,\"æ¨èç¨‹åº¦\":3,\"æ€§ä»·æ¯”\":3,\"æŠ˜æ‰£åŠ›åº¦\":0,\"è£…ä¿®æƒ…å†µ\":3,\"å˜ˆæ‚æƒ…å†µ\":0,\"å°±é¤ç©ºé—´\":3,\"å«ç”Ÿæƒ…å†µ\":3,\"åˆ†é‡\":3,\"å£æ„Ÿ\":3,\"å¤–è§‚\":3,\"æ¨èç¨‹åº¦\":3,\"å†æ¬¡æ¶ˆè´¹çš„æ„æ„¿\":3}<eoa>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import TextStreamer\n",
    "def predict(text,instruction):\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "    prompt = instruction + text + '\\nè¾“å‡ºï¼š'\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:0')\n",
    "    output = model.generate(**inputs, max_new_tokens=1024,\n",
    "                            # do_sample=True,\n",
    "                            # repetition_penalty=1.1, \n",
    "                            temperature=0.8,\n",
    "                            top_p=0.8,\n",
    "                            eos_token_id=(2, 103028),\n",
    "                            # begin_suppress_tokens=[tokenizer.eos_token_id],\n",
    "                            streamer=streamer\n",
    "                            )\n",
    "    output = output[0][inputs.input_ids.shape[1]:] # è¿™æ ·å¯ä»¥é˜²æ­¢è¾“å‡ºpromptéƒ¨åˆ†\n",
    "    return tokenizer.decode(output,skip_special_tokens=True)\n",
    "\n",
    "text = \"é¦–å…ˆæ˜¯ç¯å¢ƒå¾ˆèˆ’æœï¼Œæˆ‘å»è¿‡é™„è¿‘ä¸å°‘å’–å•¡é¦†ï¼Œåƒå›å£°ï¼Œå’–å•¡ç‹å­ä¸€å·åº—ï¼Œæ˜Ÿå·´å…‹ç­‰ç­‰ï¼Œæ„Ÿè§‰ä»–ä»¬å®¶çš„ç¯å¢ƒç»å¯¹æ˜¯é¡¶å°–çš„ä¹‹ä¸€ã€‚\\\n",
    "æ˜Ÿå·´å…‹é€‚åˆå•†åŠ¡äººå£«å–æ¯å’–å•¡å°æ†©ä¸€ä¼šï¼Œå›å£°é€‚åˆå®‰å®‰é™é™çš„å–å’–å•¡è¯»ä¹¦åº¦è¿‡ä¸€ä¸ªä¸‹åˆã€‚è¿™é‡Œæ€ä¹ˆè¯´å‘¢ï¼Œä»‹äºä¸¤è€…ä¹‹é—´å§ï¼Œæœ‰å’–å•¡æœ‰è¥¿é¤ï¼Œ\\\n",
    "    åœ¨è¿™æ™ƒä¸€å¤©éƒ½æ²¡é—®é¢˜ã€‚å¯¹äºæˆ‘è¿™å•çº¯å­¦ä¹ ç‹—æ¥è¯´ï¼ŒéŸ³ä¹å£°éŸ³æœ‰ç‚¹å¤§ï¼Œå¦å¤–æ¯ä¸ªæ¡Œå­éƒ½æœ‰ç”µæºï¼Œä½†æ˜¯macç”µè„‘çš„ç”µæºå¾ˆä¸å¥½æ’ã€‚\\\n",
    "        ç‚¹äº†ä¸€æ¯é¦™è‰æ‹¿é“ï¼Œè§‰å¾—æœ‰ç‚¹ç”œï¼Œä¸è¿‡æœ‰å…è´¹çš„æŸ æª¬æ°´ï¼Œå¯ä»¥è§£æ¸´ã€‚å›¾ä¸­æ˜¯èŠå£«ç‰ç±³åœ†é¥¼ï¼ŒèŠå£«ç”¨æ–™å¾ˆè¶³ï¼Œä¸ªäººæ„Ÿè§‰ä¸é”™ã€‚\\\n",
    "            æœ‰æ—¶é—´çš„è¯ï¼Œåœ¨è¿™é‡Œå–å–å’–å•¡ï¼Œè¯»è¯»ä¹¦åº¦è¿‡ä¸€ä¸ªä¸‹åˆçœŸæ˜¯å¤ªèˆ’æœäº†ã€‚\"\n",
    "\n",
    "instruction = \"ç»™å®šä¸€æ®µæ–‡å­—ï¼Œè¯·ä½ è¿›è¡Œç»†ç²’åº¦æ–¹é¢æƒ…æ„Ÿåˆ†æï¼Œå…·ä½“åŒ…æ‹¬ä»¥ä¸‹è¿™äº›æ–¹é¢ï¼šäº¤é€šæ˜¯å¦ä¾¿åˆ©,è·ç¦»å•†åœˆè¿œè¿‘,æ˜¯å¦å®¹æ˜“å¯»æ‰¾,æ’é˜Ÿç­‰å€™æ—¶é—´,æœåŠ¡äººå‘˜æ€åº¦,æ˜¯å¦å®¹æ˜“åœè½¦,ç‚¹èœ/ä¸Šèœé€Ÿåº¦,ä»·æ ¼æ°´å¹³,æ€§ä»·æ¯”,æŠ˜æ‰£åŠ›åº¦,è£…ä¿®æƒ…å†µ,å˜ˆæ‚æƒ…å†µ,å°±é¤ç©ºé—´,å«ç”Ÿæƒ…å†µ,åˆ†é‡,å£æ„Ÿ,å¤–è§‚,æ¨èç¨‹åº¦,æœ¬æ¬¡æ¶ˆè´¹æ„Ÿå—,å†æ¬¡æ¶ˆè´¹çš„æ„æ„¿ã€‚è¯·æŠ½å–å‡ºæ‰€æœ‰æ–¹é¢çš„æƒ…æ„Ÿå€¾å‘ï¼šæ­£å‘:1, ä¸­æ€§:0, è´Ÿå‘:-1, æœªæåŠ:-2ï¼Œç”¨jsonæ ¼å¼è¿”å›ç»“æœã€‚ è¯„è®ºå¦‚ä¸‹ï¼š\"\n",
    "output = predict(text, instruction)\n",
    "\n",
    "# json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"äº¤é€šæ˜¯å¦ä¾¿åˆ©\": \"æœªæåŠ\", \"è·ç¦»å•†åœˆè¿œè¿‘\": \"æœªæåŠ\", \"æ˜¯å¦å®¹æ˜“å¯»æ‰¾\": \"æœªæåŠ\", \"æ’é˜Ÿç­‰å€™æ—¶é—´\": \"æœªæåŠ\", \"æœåŠ¡äººå‘˜æ€åº¦\": \"æœªæåŠ\", \"æ˜¯å¦å®¹æ˜“åœè½¦\": \"æœªæåŠ\", \"ç‚¹èœ/ä¸Šèœé€Ÿåº¦\": \"æœªæåŠ\", \"ä»·æ ¼æ°´å¹³\": \"æœªæåŠ\", \"æ€§ä»·æ¯”\": \"æœªæåŠ\", \"æŠ˜æ‰£åŠ›åº¦\": \"æœªæåŠ\", \"è£…ä¿®æƒ…å†µ\": \"æ­£é¢\", \"å˜ˆæ‚æƒ…å†µ\": \"ä¸­æ€§\", \"å°±é¤ç©ºé—´\": \"æ­£é¢\", \"å«ç”Ÿæƒ…å†µ\": \"æ­£é¢\", \"åˆ†é‡\": \"æœªæåŠ\", \"å£æ„Ÿ\": \"ä¸­æ€§\", \"å¤–è§‚\": \"æœªæåŠ\", \"æ¨èç¨‹åº¦\": \"æœªæåŠ\", \"æœ¬æ¬¡æ¶ˆè´¹æ„Ÿå—\": \"æ­£é¢\", \"å†æ¬¡æ¶ˆè´¹çš„æ„æ„¿\": \"æ­£é¢\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'äº¤é€šæ˜¯å¦ä¾¿åˆ©': 'æœªæåŠ',\n",
       " 'è·ç¦»å•†åœˆè¿œè¿‘': 'æœªæåŠ',\n",
       " 'æ˜¯å¦å®¹æ˜“å¯»æ‰¾': 'æœªæåŠ',\n",
       " 'æ’é˜Ÿç­‰å€™æ—¶é—´': 'æœªæåŠ',\n",
       " 'æœåŠ¡äººå‘˜æ€åº¦': 'æœªæåŠ',\n",
       " 'æ˜¯å¦å®¹æ˜“åœè½¦': 'æœªæåŠ',\n",
       " 'ç‚¹èœ/ä¸Šèœé€Ÿåº¦': 'æœªæåŠ',\n",
       " 'ä»·æ ¼æ°´å¹³': 'æœªæåŠ',\n",
       " 'æ€§ä»·æ¯”': 'æœªæåŠ',\n",
       " 'æŠ˜æ‰£åŠ›åº¦': 'æœªæåŠ',\n",
       " 'è£…ä¿®æƒ…å†µ': 'æ­£é¢',\n",
       " 'å˜ˆæ‚æƒ…å†µ': 'ä¸­æ€§',\n",
       " 'å°±é¤ç©ºé—´': 'æ­£é¢',\n",
       " 'å«ç”Ÿæƒ…å†µ': 'æ­£é¢',\n",
       " 'åˆ†é‡': 'æœªæåŠ',\n",
       " 'å£æ„Ÿ': 'ä¸­æ€§',\n",
       " 'å¤–è§‚': 'æœªæåŠ',\n",
       " 'æ¨èç¨‹åº¦': 'æœªæåŠ',\n",
       " 'æœ¬æ¬¡æ¶ˆè´¹æ„Ÿå—': 'æ­£é¢',\n",
       " 'å†æ¬¡æ¶ˆè´¹çš„æ„æ„¿': 'æ­£é¢'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_path = '../XiaoWen/LLM-Tuning-master/weights/aspect_sentiment_jsonfy_output_10k_plus-internlm-chat-7b'\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "\n",
    "json.loads(predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! é‡è¦ç»éªŒ\n",
    "generateå‚æ•°çš„å½±å“å·¨å¤§ï¼Œåº”è¯¥è·Ÿ modeling_xxx.py ä¸­é»˜è®¤ä½¿ç”¨çš„å‚æ•°å¯¹åº”ï¼Œèƒ½ä¿è¯æ¯”è¾ƒå¥½çš„æ•ˆæœï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"äº¤é€šæ˜¯å¦ä¾¿åˆ©\": \"æœªæåŠ\", \"è·ç¦»å•†åœˆè¿œè¿‘\": \"æœªæåŠ\", \"æ˜¯å¦å®¹æ˜“å¯»æ‰¾\": \"æœªæåŠ\", \"æ’é˜Ÿç­‰å€™æ—¶é—´\": \"æœªæåŠ\", \"æœåŠ¡äººå‘˜æ€åº¦\": \"æœªæåŠ\", \"æ˜¯å¦å®¹æ˜“åœè½¦\": \"æœªæåŠ\", \"ç‚¹èœ/ä¸Šèœé€Ÿåº¦\": \"æ­£é¢\", \"ä»·æ ¼æ°´å¹³\": \"æœªæåŠ\", \"æ€§ä»·æ¯”\": \"æœªæåŠ\", \"æŠ˜æ‰£åŠ›åº¦\": \"æœªæåŠ\", \"è£…ä¿®æƒ…å†µ\": \"æœªæåŠ\", \"å˜ˆæ‚æƒ…å†µ\": \"æœªæåŠ\", \"å°±é¤ç©ºé—´\": \"æœªæåŠ\", \"å«ç”Ÿæƒ…å†µ\": \"æœªæåŠ\", \"åˆ†é‡\": \"æœªæåŠ\", \"å£æ„Ÿ\": \"æ­£é¢\", \"å¤–è§‚\": \"æœªæåŠ\", \"æ¨èç¨‹åº¦\": \"æœªæåŠ\", \"æœ¬æ¬¡æ¶ˆè´¹æ„Ÿå—\": \"æ­£é¢\", \"å†æ¬¡æ¶ˆè´¹çš„æ„æ„¿\": \"æœªæåŠ\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ç‚¹èœ/ä¸Šèœé€Ÿåº¦': 'æ­£é¢', 'å£æ„Ÿ': 'æ­£é¢', 'æœ¬æ¬¡æ¶ˆè´¹æ„Ÿå—': 'æ­£é¢'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"å¾ˆå–œæ¬¢è¿™å®¶çš„é¤ï¼Œå‘³é“æ­£å®—ï¼Œå£æ„Ÿå¾ˆå¥½ï¼Œæ˜¯æˆ‘å–œæ¬¢çš„å‘³é“ï¼Œæ»¡æ„å€¼å¾—æ¨èåŒ…è£…ä»”ç»†ï¼Œ\\\n",
    "    å‘³é“å¾ˆä¸é”™ï¼Œå°å“¥é€çš„ä¹Ÿå¾ˆå¿«ï¼Œç‚¹èµï¼å‘³é“å¾ˆæƒŠå–œï¼Œå£å‘³å¾ˆèµğŸ‘ç›¸å½“çš„æ»¡è¶³ï¼\"\n",
    "output_d = json.loads(predict(text))\n",
    "\n",
    "output_d = {k:v for k,v in output_d.items() if v != 'æœªæåŠ'}\n",
    "output_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ç­”æ¡ˆï¼š{\"pos\":0,\"neg\":1,\"neu\":0,\"text\": \"å–è¿‡æœ€éš¾å–çš„å¥¶èŒ¶ï¼Œä¸€è‚¡æ°´çš„å‘³é“ï¼Œæ ¹æœ¬å°±å–ä¸åˆ°å¥¶èŒ¶å‘³ï¼Œå…¨å€’äº†ï¼Œæµªè´¹ã€‚èŠ‹æ³¥å°±ä¸€ç‚¹ç‚¹ï¼Œçç å°±6å°é¢—ã€‚æœæ°”äº†ï¼å·®è¯„ï¼Œå·®è¯„ã€‚\"}<eoa>\n",
      "\n",
      "ç­”æ¡ˆï¼š{\"pos\":0,\"neg\":1,\"neu\":0,\"text\": \"å–è¿‡æœ€éš¾å–çš„å¥¶èŒ¶ï¼Œä¸€è‚¡æ°´çš„å‘³é“ï¼Œæ ¹æœ¬å°±å–ä¸åˆ°å¥¶èŒ¶å‘³ï¼Œå…¨å€’äº†ï¼Œæµªè´¹ã€‚èŠ‹æ³¥å°±ä¸€ç‚¹ç‚¹ï¼Œçç å°±6å°é¢—ã€‚æœæ°”äº†ï¼å·®è¯„ï¼Œå·®è¯„ã€‚\"}<eoa>\n"
     ]
    }
   ],
   "source": [
    "with model.disable_adapter(): # ç¦ç”¨ lora\n",
    "    output = predict(text)\n",
    "    try:\n",
    "        output_d = json.loads(output)\n",
    "        output_d = {k:v for k,v in output_d.items() if v != 'æœªæåŠ'}\n",
    "        print(output_d)\n",
    "    except:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
