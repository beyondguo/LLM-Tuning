# Tuning LLMs with no tears ğŸ’¦.


ğŸ’» å¯å¤ç°çš„å°é¡¹ç›®ï¼š
- [**ChatBaichuan**ï¼šåŸºäº HC3 æ•°æ®é›†è®© ç™¾å·å¤§æ¨¡å‹ï¼ˆbaichuan-7Bï¼‰æœ‰å¯¹è¯èƒ½åŠ›ï¼](/projects/ChatBaichuan-HC3/)
- [ã€å¨±ä¹å‘ã€‘**RulaiGPT**ï¼šå¦‚æ¥\~è¯¶ï¼Œå®ƒçœŸæ¥äº†å—ï¼Ÿå¦‚\~æ¥\~ï¼ˆæ‹æ¡Œï¼ï¼‰](/projects/RulaiGPT/)


ğŸ’¬ ç›¸å…³è®¨è®ºåŒºï¼š
- [å®˜æ–¹ WeChat è®¨è®ºç¾¤/WeChat Group](https://github.com/beyondguo/LLM-Tuning/discussions/23)
- [LLM å¾®è°ƒä¸­çš„â€œç¾éš¾æ€§é—å¿˜â€é—®é¢˜ä¸“é¢˜è®¨è®ºåŒº/Catastrophic Forgetting Discussion](https://github.com/beyondguo/LLM-Tuning/discussions/24)


ğŸ¤– ç›®å‰æ”¯æŒï¼š
- æ¸…å [ChatGLM2-6B](https://huggingface.co/THUDM/chatglm2-6b) çš„ LoRA å¾®è°ƒ (New!ğŸ”¥)
- ç™¾å·æ™ºèƒ½ [baichuan-7B](https://huggingface.co/baichuan-inc/baichuan-7B) çš„ LoRA å¾®è°ƒ
- æ¸…å [ChatGLM-6B](https://huggingface.co/THUDM/chatglm-6b) çš„ LoRA å¾®è°ƒ


ğŸ¯ ä¸¤è¡Œä»£ç å¼€å¯è®­ç»ƒï¼š
- æ•°æ®é›†åˆ†è¯é¢„å¤„ç†ï¼š`sh tokenize.sh`ï¼Œå¯¹æ¯”ä¸åŒçš„ LLMï¼Œéœ€åœ¨ tokenize.sh æ–‡ä»¶é‡Œåˆ‡æ¢ model_checkpoint å‚æ•°
- å¼€å¯ LoRA å¾®è°ƒï¼š`sh train.sh`ï¼Œå¯¹äºä¸åŒçš„ LLMï¼Œéœ€åˆ‡æ¢ä¸åŒçš„ python æ–‡ä»¶æ¥æ‰§è¡Œï¼š
    - ChatGLM-6B åº”ä½¿ç”¨ `chatglm_lora_tuning.py`
    - ChatGLM2-6B åº”ä½¿ç”¨ `chatglm2_lora_tuning.py`
    - baichuan-7B åº”ä½¿ç”¨ `baichuan_lora_tuning.py`

---


**ç¯å¢ƒå‡†å¤‡**ï¼š\
`pip install transformers datasets accelerate sentencepiece tensorboard peft`\
ç›®å‰æµ‹è¯•çš„ç¯å¢ƒä¸ºï¼š
```
- Python 3.9.16
- torch, Version: 2.0.1
- transformers, Version: 4.29.1
- datasets, Version: 2.12.0
- accelerate, Version: 0.19.0
- peft, Version: 0.3.0
- sentencepiece, Version: 0.1.99
- tensorboard, Version: 2.13.0
```

## æ•™ç¨‹ï¼š
ä¸‹é¢çš„æ•™ç¨‹ä»¥åŠä»£ç ä½¿ç”¨ `ChatGLM-6B` ä½œä¸ºä¾‹å­ï¼Œå¦‚æœæ›´æ¢å…¶ä»–æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ç•¥å¾®ä¿®æ”¹å…·ä½“æ–‡ä»¶ä»£ç ã€‚

### 1. æŒ‡ä»¤å¾®è°ƒæ•°æ®å‡†å¤‡ Instruction Data Preparation
**åŸå§‹æ–‡ä»¶çš„å‡†å¤‡**

æŒ‡ä»¤å¾®è°ƒæ•°æ®ä¸€èˆ¬æœ‰è¾“å…¥å’Œè¾“å‡ºä¸¤éƒ¨åˆ†ï¼Œè¾“å…¥æ˜¯ç‰¹å®šçš„contentåŠ ä¸Šinstructionï¼Œè¿™é‡Œæˆ‘ä»¬å°†äºŒè€…ç›´æ¥æ‹¼åœ¨ä¸€èµ·ï¼Œä¸å•ç‹¬åŒºåˆ†ï¼›è¾“å‡ºåˆ™æ˜¯å¸Œæœ›æ¨¡å‹çš„å›ç­”ã€‚
æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨`json`çš„æ ¼å¼åœ¨æ•´ç†æ•°æ®ï¼Œå¯ä»¥è‡ªå®šä¹‰è¾“å‡ºè¾“å‡ºçš„å­—æ®µåï¼Œä¾‹å¦‚ä¸‹é¢çš„ä¾‹å­ä¸­æˆ‘ä½¿ç”¨çš„æ˜¯`q`å’Œ`a`ä»£è¡¨æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºï¼š
```json
{"q": "è¯·è®¡ç®—ï¼š39 * 0 = ä»€ä¹ˆï¼Ÿ", "a": "è¿™æ˜¯ç®€å•çš„ä¹˜æ³•è¿ç®—ï¼Œ39ä¹˜ä»¥0å¾—åˆ°çš„æ˜¯0"}
{"q": "é¢˜ç›®ï¼š51/186çš„ç­”æ¡ˆæ˜¯ä»€ä¹ˆ?", "a": "è¿™æ˜¯ç®€å•çš„é™¤æ³•è¿ç®—ï¼Œ51é™¤ä»¥186å¤§æ¦‚ä¸º0.274"}
{"q": "é¹¿å¦ˆå¦ˆä¹°äº†24ä¸ªè‹¹æœï¼Œå¥¹æƒ³å¹³å‡åˆ†ç»™å¥¹çš„3åªå°é¹¿åƒï¼Œæ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°å‡ ä¸ªè‹¹æœï¼Ÿ", "a": "é¹¿å¦ˆå¦ˆä¹°äº†24ä¸ªè‹¹æœï¼Œå¹³å‡åˆ†ç»™3åªå°é¹¿åƒï¼Œé‚£ä¹ˆæ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°çš„è‹¹æœæ•°å°±æ˜¯æ€»è‹¹æœæ•°é™¤ä»¥å°é¹¿çš„åªæ•°ã€‚\n24Ã·3=8\næ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°8ä¸ªè‹¹æœã€‚æ‰€ä»¥ï¼Œç­”æ¡ˆæ˜¯æ¯åªå°é¹¿å¯ä»¥åˆ†åˆ°8ä¸ªè‹¹æœã€‚"}
...
```
æ•´ç†å¥½æ•°æ®åï¼Œä¿å­˜ä¸º`.json`æˆ–è€…`.jsonl`æ–‡ä»¶ï¼Œç„¶åæ”¾å…¥ç›®å½•ä¸­çš„`data/`æ–‡ä»¶å¤¹ä¸­ã€‚

**å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯**

ä¸ºäº†é¿å…æ¯æ¬¡è®­ç»ƒçš„æ—¶å€™éƒ½è¦é‡æ–°å¯¹æ•°æ®é›†åˆ†è¯ï¼Œæˆ‘ä»¬å…ˆåˆ†å¥½è¯å½¢æˆç‰¹å¾åä¿å­˜æˆå¯ç›´æ¥ç”¨äºè®­ç»ƒçš„æ•°æ®é›†ã€‚

ä¾‹å¦‚ï¼Œ
- æˆ‘ä»¬çš„åŸå§‹æŒ‡ä»¤å¾®è°ƒæ–‡ä»¶ä¸ºï¼š`data/` æ–‡ä»¶å¤¹ä¸‹çš„ `simple_math_4op.json` æ–‡ä»¶
- è¾“å…¥å­—æ®µä¸º`q`ï¼Œè¾“å‡ºå­—æ®µä¸º`a`
- å¸Œæœ›ç»è¿‡ tokenize ä¹‹åä¿å­˜åˆ° `data/tokenized_data/` ä¸‹åä¸º `simple_math_4op` çš„æ–‡ä»¶å¤¹ä¸­
- è®¾å®šæ–‡æœ¬æœ€å¤§ç¨‹åº¦ä¸º 2000

åˆ™æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸‹é¢è¿™æ®µå‘½ä»¤(å³`tokenize.sh`æ–‡ä»¶)è¿›è¡Œå¤„ç†ï¼š
```shell
CUDA_VISIBLE_DEVICES=0,1 python tokenize_dataset_rows.py \
    --model_checkpoint THUDM/chatglm-6b \
    --input_file simple_math_4op.json \
    --prompt_key q \
    --target_key a \
    --save_name simple_math_4op \
    --max_seq_length 2000 \
    --skip_overlength False
```
å¤„ç†å®Œæ¯•ä¹‹åï¼Œæˆ‘ä»¬ä¼šåœ¨ `data/tokenized_data/` ä¸‹å‘ç°åä¸º `simple_math_4op` çš„æ–‡ä»¶å¤¹ï¼Œè¿™å°±æ˜¯ä¸‹ä¸€æ­¥ä¸­æˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒçš„æ•°æ®ã€‚


### 2. ä½¿ç”¨ `LoRA` å¾®è°ƒ

å¾—åˆ° tokenize ä¹‹åçš„æ•°æ®é›†ï¼Œå°±å¯ä»¥ç›´æ¥è¿è¡Œ `chatglm_lora_tuning.py` æ¥è®­ç»ƒ LoRA æ¨¡å‹äº†ï¼Œå…·ä½“å¯è®¾ç½®çš„ä¸»è¦å‚æ•°åŒ…æ‹¬ï¼š
- `tokenized_dataset`, åˆ†è¯åçš„æ•°æ®é›†ï¼Œå³åœ¨ data/tokenized_data/ åœ°å€ä¸‹çš„æ–‡ä»¶å¤¹åç§°
- `lora_rank`, è®¾ç½® LoRA çš„ç§©ï¼Œæ¨èä¸º4æˆ–8ï¼Œæ˜¾å­˜å¤Ÿçš„è¯ä½¿ç”¨8
- `per_device_train_batch_size`, æ¯å— GPU ä¸Šçš„ batch size
- `gradient_accumulation_steps`, æ¢¯åº¦ç´¯åŠ ï¼Œå¯ä»¥åœ¨ä¸æå‡æ˜¾å­˜å ç”¨çš„æƒ…å†µä¸‹å¢å¤§ batch size
- `max_steps`, è®­ç»ƒæ­¥æ•°
- `save_steps`, å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡
- `save_total_limit`, ä¿å­˜å¤šå°‘ä¸ªcheckpoint
- `logging_steps`, å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒæƒ…å†µ(loss, lr, etc.)
- `output_dir`, æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ°å€

ä¾‹å¦‚æˆ‘ä»¬çš„æ•°æ®é›†ä¸º simple_math_4opï¼Œå¸Œæœ›ä¿å­˜åˆ° weights/simple_math_4op ï¼Œåˆ™æ‰§è¡Œä¸‹é¢å‘½ä»¤(å³`train.sh`æ–‡ä»¶)ï¼š
```shell
CUDA_VISIBLE_DEVICES=2,3 python chatglm_lora_tuning.py \
    --tokenized_dataset simple_math_4op \
    --lora_rank 8 \
    --per_device_train_batch_size 10 \
    --gradient_accumulation_steps 1 \
    --max_steps 100000 \
    --save_steps 200 \
    --save_total_limit 2 \
    --learning_rate 1e-4 \
    --fp16 \
    --remove_unused_columns false \
    --logging_steps 50 \
    --output_dir weights/simple_math_4op
```

è®­ç»ƒå®Œä¹‹åï¼Œå¯ä»¥åœ¨ output_dir ä¸­æ‰¾åˆ° LoRA çš„ç›¸å…³æ¨¡å‹æƒé‡ï¼Œä¸»è¦æ˜¯`adapter_model.bin`å’Œ`adapter_config.json`ä¸¤ä¸ªæ–‡ä»¶ã€‚


> å¦‚ä½•æŸ¥çœ‹ tensorboardï¼š
- åœ¨ output_dir ä¸­æ‰¾åˆ° runs æ–‡ä»¶å¤¹ï¼Œå¤åˆ¶å…¶ä¸­æ—¥æœŸæœ€å¤§çš„æ–‡ä»¶å¤¹çš„åœ°å€ï¼Œå‡è®¾ä¸º `your_log_path`
- æ‰§è¡Œ `tensorboard --logdir your_log_path` å‘½ä»¤ï¼Œå°±ä¼šåœ¨ http://localhost:6006/ ä¸Šå¼€å¯tensorboard
- å¦‚æœæ˜¯åœ¨æœåŠ¡å™¨ä¸Šå¼€å¯ï¼Œåˆ™è¿˜éœ€è¦åšç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚æ¨èä½¿ç”¨ VSCode åœ¨æœåŠ¡å™¨ä¸Šå†™ä»£ç ï¼Œå¯ä»¥è‡ªåŠ¨å¸®ä½ è¿›è¡Œç«¯å£æ˜ å°„ã€‚
- å¦‚æœè¦è‡ªå·±æ‰‹åŠ¨è¿›è¡Œç«¯å£æ˜ å°„ï¼Œå…·ä½“æ–¹å¼æ˜¯åœ¨ä½¿ç”¨ ssh ç™»å½•æ—¶ï¼Œåé¢åŠ ä¸Š `-L 6006:127.0.0.1:6006` å‚æ•°ï¼Œå°†æœåŠ¡å™¨ç«¯çš„6006ç«¯å£æ˜ å°„åˆ°æœ¬åœ°çš„6006ç«¯å£ã€‚


### 3. æ‹¿èµ° LoRA å°å°çš„æ–‡ä»¶ï¼Œåˆ°ä½ æœ¬åœ°çš„å¤§æ¨¡å‹ä¸ŠåŠ è½½å¹¶æ¨ç†

æˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„ output_dir æ‰“åŒ…å¸¦èµ°ï¼Œå‡è®¾æ–‡ä»¶å¤¹ä¸º `weights/simple_math_4op`ï¼Œ å…¶ä¸­ï¼ˆè‡³å°‘ï¼‰åŒ…å« `adapter_model.bin` å’Œ `adapter_config.json` ä¸¤ä¸ªæ–‡ä»¶ï¼Œåˆ™æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„æ–¹å¼ç›´æ¥åŠ è½½ï¼Œå¹¶æ¨ç†

```python
from peft import PeftModel
from transformers import AutoTokenizer, AutoModel
import torch

device = torch.device(1)
# åŠ è½½åŸå§‹ LLM
model_path = "THUDM/chatglm-6b"
model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().to(device)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model.chat(tokenizer, "ä½ å¥½", history=[])


# ç»™åŸå§‹ LLM å®‰è£…ä¸Šä½ çš„ LoRA tool
model = PeftModel.from_pretrained(model, "weights/simple_math_4op").half()
model.chat(tokenizer, "ä½ å¥½", history=[])
```

ç†è®ºä¸Šï¼Œå¯ä»¥é€šè¿‡å¤šæ¬¡æ‰§è¡Œ `model = PeftModel.from_pretrained(model, "weights/simple_math_4op").half()` çš„æ–¹å¼ï¼ŒåŠ è½½å¤šä¸ª LoRA æ¨¡å‹ï¼Œä»è€Œæ··åˆä¸åŒToolçš„èƒ½åŠ›ï¼Œä½†å®é™…æµ‹è¯•çš„æ—¶å€™ï¼Œç”±äºæš‚æ—¶è¿˜ä¸æ”¯æŒè®¾ç½®ä¸åŒ LoRA weightsçš„æƒé‡ï¼Œå¾€å¾€æ•ˆæœä¸å¤ªå¥½ï¼Œå­˜åœ¨è¦†ç›–æˆ–è€…é—å¿˜çš„æƒ…å†µã€‚


---

### Acknowledgement
- é¦–å…ˆæœ€æ„Ÿè°¢çš„æ˜¯ ğŸ¤—Huggingface å›¢é˜Ÿå¼€æºçš„ [peft](https://github.com/huggingface/peft) å·¥å…·åŒ…ï¼Œæ‡‚çš„éƒ½æ‡‚ï¼
- ChatGLM çš„ LoRA å¾®è°ƒä»£ç ä¸»è¦åŸºäº [ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) é¡¹ç›®ä¸­çš„ LoRA å¾®è°ƒéƒ¨åˆ†ä¿®æ”¹è€Œæ¥ï¼›
- baichuan-7B å¾®è°ƒéƒ¨åˆ†ï¼Œå‚è€ƒäº† [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning/issues/43) é¡¹ç›®ä¸­çš„è§£å†³æ–¹æ¡ˆï¼›

å¯¹è¿™äº›ä¼˜ç§€å¼€æºé¡¹ç›®è¡¨ç¤ºæ„Ÿè°¢ï¼